# Bike_Sharing_Prediction

The first step was to import the data and then split it so that the train set contained the first year and the first three quarters  of the second year, and the test set contained the fourth quarter of the second year.

Exploratory Data Analysis
The data was then visualized by plotting the total count vs hour for each day of the week. This was done to see the influence of hour and week day on bike usage.
Then two more graphs were plotted with the same format as the first, but this time users were divided into casual and registered. This visualization showed that casual users are more active during weekends and the middle of the day, while registered users are more active during weekdays, specifically during commuting hours. It was also noted that total count as increased between years 1 and 2.
The next step was to ensure data quality. This involved checking for duplicates and removing them, checking for null values (none were found), checking for any data errors (none were found) and checking for outliers. For the outliers, a boxplot was used to show the entire dataset, and then two more boxplots were created, one for 2011 and the other for 2012. In this way, most of the outliers were to be found in 2011. The decision was made to keep these, in order to avoid losing important information from the dataset.
A heatmap was then produced in order to reveal correlations between the predictor variables and the target variable. In this way, temp and atemp were found to be highly correlated, so the decision was taken to drop temp, since ‘feeling temperature’ was deemed to be a more important factor in a customer’s decision-making process than overall temperature. Since casual and registered are both related to total count, in the sense that they both directly contribute to total count, the decision was taken to drop them. Using them for predictions is similar to using information from the future, which would rarely be possible in a real-world scenario. Meanwhile four new features were added. These included: ratio of atemp to humidity, ratio of wind speed to atemp, product of wind speed and humidity, and product of wind speed and humidity.

Machine Learning
First an elastic net was run. A 10-fold cross validation was used with various L1 ratios to find the best alpha hyperparameter and L1 ratio. The best results were an L1 of 1 and an alpha ratio of 0.023. The L1 ratio implied that a pure lasso model was the best option, and so this was run on the test set obtaining an R2 of 0.871, and an RMSE of 72.523. The procedure was repeated, but this time the registered column was added to the data. A lasso was again run, this time obtaining and R2 of 0.989 and an RMSE of 21.532; this was obviously a great improvement on the previous results, which was due to using the future information from the target variable in order to predict it; again, this would likely not be available in a real world situation.
Next, a decision tree was used to run the same process, with a grid search used to find the best hyperparameters using cross validation. With the registered column excluded, an R2 of 0.695 and an RMSE of 111.362 was obtained; with it included the R2 was 0.977 and the RMSE was 30.312.
In conclusion, the lasso performed significantly better than the decision tree.
A random forest was then used with a grid search to find the best hyperparameters. Random forest was used to calculate the feature importance, with the most important being the ratio of atemp to humidity. The next most important features are the time-based ones; this fit with previous findings, where the number of bikes used depended on the time and day of the week. Running the random forest on the test set obtained an R2 of 0.776 and an RMSE of 95.24, thus this performs poorly in comparison to the Lasso.

Conclusion
Stepping back from the problem, the dataset is a typical time series dataset. In this project, machine learning models were used to forecast but in reality, different methods would be more appropriate, since they would be able to take into account the influence of the past on the future. In the process of carrying out this assignment, the work was divided as follows. All of the team members took part in the feature engineering by creating new features and testing them, the four best features were then selected. The machine learning process was conducted as a team, with all team members puzzling through the correct procedures together, coding different parts.
